@misc{VisionMiddleburyEdu,
  title = {Vision.Middlebury.Edu/Stereo/Data},
  howpublished = {https://vision.middlebury.edu/stereo/data/},
  file = {/Users/newt/Zotero/storage/W3B6P5NJ/data.html}
}
@article{beyerAmesStereoPipeline2018,
  title = {The {{Ames Stereo Pipeline}}: {{NASA}}'s {{Open Source Software}} for {{Deriving}} and {{Processing Terrain Data}}},
  shorttitle = {The {{Ames Stereo Pipeline}}},
  author = {Beyer, Ross A. and Alexandrov, Oleg and McMichael, Scott},
  year = {2018},
  journal = {Earth and Space Science},
  volume = {5},
  number = {9},
  pages = {537--548},
  issn = {2333-5084},
  doi = {10.1029/2018EA000409},
  urldate = {2023-11-23},
  abstract = {The NASA Ames Stereo Pipeline is a suite of free and open source automated geodesy and stereogrammetry tools designed for processing stereo images captured from satellites (around Earth and other planets), robotic rovers, aerial cameras, and historical images, with and without accurate camera pose information. It produces cartographic products, including digital terrain models, ortho-projected images, 3-D models, and bundle-adjusted networks of cameras. Ames Stereo Pipeline's data products are suitable for science analysis, mission planning, and public outreach.},
  copyright = {\textcopyright 2018. The Authors.},
  langid = {english},
  file = {/Users/newt/Zotero/storage/E9KCCYUH/Beyer et al. - 2018 - The Ames Stereo Pipeline NASA's Open Source Softw.pdf;/Users/newt/Zotero/storage/6GKQFTZX/2018EA000409.html}
}
@incollection{lindebergScaleInvariantFeature2012,
  title = {Scale {{Invariant Feature Transform}}},
  booktitle = {Scholarpedia},
  author = {Lindeberg, Tony},
  year = {2012},
  month = may,
  volume = {7},
  doi = {10.4249/scholarpedia.10491},
  abstract = {Scale Invariant Feature Transform (SIFT) is an image descriptor for image-based matching developed by David Lowe (1999, 2004). This descriptor as well as related image descriptors are used for a large number of purposes in computer vision related to point matching between different views of a 3-D scene and view-based object recognition. The SIFT descriptor is invariant to translations, rotations and scaling transformations in the image domain and robust to moderate perspective transformations and illumination variations. Experimentally, the SIFT descriptor has been proven to be very useful in practice for image matching and object recognition under real-world conditions. In its original formulation, the SIFT descriptor comprised a method for detecting interest points from a grey-level image at which statistics of local gradient directions of image intensities were accumulated to give a summarizing description of the local image structures in a local neighbourhood around each interest point, with the intention that this descriptor should be used for matching corresponding interest points between different images. Later, the SIFT descriptor has also been applied at dense grids (dense SIFT) which have been shown to lead to better performance for tasks such as object categorization, texture classification, image alignment and biometrics . The SIFT descriptor has also been extended from grey-level to colour images and from 2-D spatial images to 2+1-D spatio-temporal video.},
  file = {/Users/newt/Zotero/storage/3R6RYEW6/Lindeberg - 2012 - Scale Invariant Feature Transform.pdf}
}

@article{fischlerRandomSampleConsensus1981,
  title = {Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography},
  shorttitle = {Random Sample Consensus},
  author = {Fischler, Martin A. and Bolles, Robert C.},
  year = {1981},
  month = jun,
  journal = {Communications of the ACM},
  volume = {24},
  number = {6},
  pages = {381--395},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/358669.358692},
  urldate = {2023-11-23},
  abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing},
  langid = {english}
}


@article{kolomogorov,
  author={Kolmogorov, V. and Zabih, R.},
  booktitle={Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001}, 
  title={Computing visual correspondence with occlusions using graph cuts}, 
  year={2001},
  volume={2},
  number={},
  pages={508-515 vol.2},
  doi={10.1109/ICCV.2001.937668}}
@article{huangDeepMVSLearningMultiview2018,
  title = {{{DeepMVS}}: {{Learning Multi-view Stereopsis}}},
  shorttitle = {{{DeepMVS}}},
  author = {Huang, Po-Han and Matzen, Kevin and Kopf, Johannes and Ahuja, Narendra and Huang, Jia-Bin},
  year = {2018},
  month = apr,
  number = {arXiv:1804.00650},
  eprint = {1804.00650},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1804.00650},
  urldate = {2023-12-31},
  abstract = {We present DeepMVS, a deep convolutional neural network (ConvNet) for multi-view stereo reconstruction. Taking an arbitrary number of posed images as input, we first produce a set of plane-sweep volumes and use the proposed DeepMVS network to predict high-quality disparity maps. The key contributions that enable these results are (1) supervised pretraining on a photorealistic synthetic dataset, (2) an effective method for aggregating information across a set of unordered images, and (3) integrating multi-layer feature activations from the pre-trained VGG-19 network. We validate the efficacy of DeepMVS using the ETH3D Benchmark. Our results show that DeepMVS compares favorably against state-of-the-art conventional MVS algorithms and other ConvNet based methods, particularly for near-textureless regions and thin structures.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/newt/Zotero/storage/9C5PDLB5/Huang et al. - 2018 - DeepMVS Learning Multi-view Stereopsis.pdf;/Users/newt/Zotero/storage/2PJZ42NJ/1804.html}
}
@article{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  year = {2020},
  month = aug,
  number = {arXiv:2003.08934},
  eprint = {2003.08934},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2003.08934},
  urldate = {2023-12-31},
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  file = {/Users/newt/Zotero/storage/4SD8D3NS/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf;/Users/newt/Zotero/storage/JZXCE4WQ/2003.html}
}

@article{shenRANSACFlowGenericTwostage2020,
  title = {{{RANSAC-Flow}}: Generic Two-Stage Image Alignment},
  shorttitle = {{{RANSAC-Flow}}},
  author = {Shen, Xi and Darmon, Fran{\c c}ois and Efros, Alexei A. and Aubry, Mathieu},
  year = {2020},
  month = jul,
  number = {arXiv:2004.01526},
  eprint = {2004.01526},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.01526},
  urldate = {2023-12-31},
  abstract = {This paper considers the generic problem of dense alignment between two images, whether they be two frames of a video, two widely different views of a scene, two paintings depicting similar content, etc. Whereas each such task is typically addressed with a domain-specific solution, we show that a simple unsupervised approach performs surprisingly well across a range of tasks. Our main insight is that parametric and non-parametric alignment methods have complementary strengths. We propose a two-stage process: first, a feature-based parametric coarse alignment using one or more homographies, followed by non-parametric fine pixel-wise alignment. Coarse alignment is performed using RANSAC on off-the-shelf deep features. Fine alignment is learned in an unsupervised way by a deep network which optimizes a standard structural similarity metric (SSIM) between the two images, plus cycle-consistency. Despite its simplicity, our method shows competitive results on a range of tasks and datasets, including unsupervised optical flow on KITTI, dense correspondences on Hpatches, two-view geometry estimation on YFCC100M, localization on Aachen Day-Night, and, for the first time, fine alignment of artworks on the Brughel dataset. Our code and data are available at http://imagine.enpc.fr/{\textasciitilde}shenx/RANSAC-Flow/},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/newt/Zotero/storage/64QB9IMV/Shen et al. - 2020 - RANSAC-Flow generic two-stage image alignment.pdf;/Users/newt/Zotero/storage/99EKPM54/2004.html}
}
@inproceedings{darmonLearningGuideLocal2020,
  title = {Learning to {{Guide Local Feature Matches}}},
  booktitle = {2020 {{International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Darmon, Fran{\c c}ois and Aubry, Mathieu and Monasse, Pascal},
  year = {2020},
  month = nov,
  eprint = {2010.10959},
  primaryclass = {cs},
  pages = {1127--1136},
  doi = {10.1109/3DV50981.2020.00123},
  urldate = {2023-12-31},
  abstract = {We tackle the problem of finding accurate and robust keypoint correspondences between images. We propose a learning-based approach to guide local feature matches via a learned approximate image matching. Our approach can boost the results of SIFT to a level similar to state-of-the-art deep descriptors, such as Superpoint, ContextDesc, or D2-Net and can improve performance for these descriptors. We introduce and study different levels of supervision to learn coarse correspondences. In particular, we show that weak supervision from epipolar geometry leads to performances higher than the stronger but more biased point level supervision and is a clear improvement over weak image level supervision. We demonstrate the benefits of our approach in a variety of conditions by evaluating our guided keypoint correspondences for localization of internet images on the YFCC100M dataset and indoor images on theSUN3D dataset, for robust localization on the Aachen day-night benchmark and for 3D reconstruction in challenging conditions using the LTLL historical image data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/newt/Zotero/storage/4QFZVZEL/Darmon et al. - 2020 - Learning to Guide Local Feature Matches.pdf;/Users/newt/Zotero/storage/WYLDW7KJ/2010.html}
}

@article{yiLIFTLearnedInvariant2016,
  title = {{{LIFT}}: {{Learned Invariant Feature Transform}}},
  shorttitle = {{{LIFT}}},
  author = {Yi, Kwang Moo and Trulls, Eduard and Lepetit, Vincent and Fua, Pascal},
  year = {2016},
  month = jul,
  number = {arXiv:1603.09114},
  eprint = {1603.09114},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1603.09114},
  urldate = {2023-12-31},
  abstract = {We introduce a novel Deep Network architecture that implements the full feature point handling pipeline, that is, detection, orientation estimation, and feature description. While previous works have successfully tackled each one of these problems individually, we show how to learn to do all three in a unified manner while preserving end-to-end differentiability. We then demonstrate that our Deep pipeline outperforms state-of-the-art methods on a number of benchmark datasets, without the need of retraining.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/newt/Zotero/storage/SXRA9CSP/Yi et al. - 2016 - LIFT Learned Invariant Feature Transform.pdf;/Users/newt/Zotero/storage/BV7F7F3W/1603.html}
}
@inproceedings{hirschmullerAccurateEfficientStereo2005a,
  title = {Accurate and Efficient Stereo Processing by Semi-Global Matching and Mutual Information},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Hirschmuller, H.},
  year = {2005},
  month = jun,
  volume = {2},
  pages = {807-814 vol. 2},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2005.56},
  urldate = {2023-12-31},
  abstract = {This paper considers the objectives of accurate stereo matching, especially at object boundaries, robustness against recording or illumination changes and efficiency of the calculation. These objectives lead to the proposed semi-global matching method that performs pixelwise matching based on mutual information and the approximation of a global smoothness constraint. Occlusions are detected and disparities determined with sub-pixel accuracy. Additionally, an extension for multi-baseline stereo images is presented. There are two novel contributions. Firstly, a hierarchical calculation of mutual information based matching is shown, which is almost as fast as intensity based matching. Secondly, an approximation of a global cost calculation is proposed that can be performed in a time that is linear to the number of pixels and disparities. The implementation requires just 1 second on typical images.},
  file = {/Users/newt/Zotero/storage/NZZCHMCH/Hirschmuller - 2005 - Accurate and efficient stereo processing by semi-g.pdf;/Users/newt/Zotero/storage/3TTW3H3E/1467526.html}
}
@article{schmidhuberDeepLearningNeural2015,
  title = {Deep {{Learning}} in {{Neural Networks}}: {{An Overview}}},
  shorttitle = {Deep {{Learning}} in {{Neural Networks}}},
  author = {Schmidhuber, Juergen},
  year = {2015},
  month = jan,
  journal = {Neural Networks},
  volume = {61},
  eprint = {1404.7828},
  primaryclass = {cs},
  pages = {85--117},
  issn = {08936080},
  doi = {10.1016/j.neunet.2014.09.003},
  urldate = {2023-12-31},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/newt/Zotero/storage/K7A3XM8Q/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf}
}

@article{lmeds,
  author={Rosin, P.L.},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)}, 
  title={Robust pose estimation}, 
  year={1999},
  volume={29},
  number={2},
  pages={297-303},
  doi={10.1109/3477.752804}}
